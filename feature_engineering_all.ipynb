{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv('../data/kzhan176/citizens_data/BrownDSI_masked_capstone_data.csv_20250401031515')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_calgorial_features(df):\n",
    "\n",
    "    '''\n",
    "    one hot encoding:\n",
    "    - masked_bank_num\n",
    "    - masked_account_type\n",
    "    - masked_product_code_grouped\n",
    "    - relationship_balance_new_account\n",
    "    - oao_flg\n",
    "    - onus_ind\n",
    "    - treasury_check_ind\n",
    "    - heloc_ind\n",
    "    '''\n",
    "    one_hot_features = ['masked_bank_num', 'masked_account_type', 'masked_product_code_grouped']\n",
    "    df = pd.get_dummies(df, columns=one_hot_features)\n",
    "    \n",
    "    df['oao_flg'] = (df['oao_flg'] == 'Y').astype(int)\n",
    "\n",
    "    cat_ftrs = ['onus_ind', 'treasury_check_ind', 'heloc_ind']\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant',fill_value='other')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False,handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[('cat', categorical_transformer, cat_ftrs)]\n",
    "    )\n",
    "    df = preprocessor.fit_transform(df)\n",
    "\n",
    "\n",
    "    '''\n",
    "    ordinal encoding\n",
    "    - bucket_days_since_open\n",
    "    - deposit_quarter\n",
    "    - deposit_dayofweek\n",
    "    '''\n",
    "    ordinal_mapping = {\n",
    "        '0-1000': 0,\n",
    "        '1000-2000': 1,\n",
    "        '2000-5000': 2,\n",
    "        '5000+': 3\n",
    "    }\n",
    "    df['bucket_days_since_open'] = df['bucket_days_since_open'].map(ordinal_mapping)\n",
    "    # fill missing values with -1\n",
    "    df['bucket_days_since_open'] = df['bucket_days_since_open'].fillna(-1)\n",
    "\n",
    "    df['deposit_dt'] = pd.to_datetime(df['deposit_dt'])\n",
    "    df['deposit_quarter'] = df['deposit_dt'].dt.quarter\n",
    "    df['deposit_dayofweek'] = df['deposit_dt'].dt.dayofweek\n",
    "    df = df.drop(columns='deposit_dt')\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def process_continous_features(df):\n",
    "    # total_deposit_amount\n",
    "    df['total_deposit_amount'] = df['total_deposit_amount'].applymap(lambda x: np.log1p(x) if x > 0 else 0)\n",
    "\n",
    "    # item_amt\n",
    "    df['item_amt'] = df['item_amt'].applymap(lambda x: np.log1p(x) if x > 0 else 0)\n",
    "\n",
    "    # relationship_balance\n",
    "    df['relationship_balance_new_account'] = (df['relationship_balance'] == -99999999).astype(int)\n",
    "\n",
    "    df['relationship_balance'] = df['relationship_balance'].replace(-99999999, np.nan)\n",
    "    median_val = df['relationship_balance'].median()\n",
    "    df['relationship_balance'] = df['relationship_balance'].fillna(median_val)\n",
    "    min_val = df['relationship_balance'].min()\n",
    "    shift = 1 - min_val if min_val <= 0 else 0\n",
    "    df['relationship_balance'] = np.log1p(df['relationship_balance'] + shift)\n",
    "\n",
    "    # rdis, max_deposit_amount30d, total_deposit_item_count\n",
    "    # drawee_sum, drawee_cnt\n",
    "    num_ftrs = ['drawee_sum', 'drawee_cnt']\n",
    "    log_num_ftrs = ['rdis', 'max_deposit_amount30d', 'total_deposit_item_count']\n",
    "\n",
    "    df['rdis'] = df['rdis'].fillna(0)\n",
    "    df['drawee_sum'] = df['drawee_sum'].fillna(0)\n",
    "    df['drawee_cnt'] = df['drawee_cnt'].fillna(0)\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "    log_numeric_transformer = Pipeline(steps=[\n",
    "        ('log', FunctionTransformer(np.log1p, feature_names_out = 'one-to-one')),\n",
    "        ('scaler', RobustScaler())])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('log_num', log_numeric_transformer, log_num_ftrs),\n",
    "            ('num', numeric_transformer, num_ftrs)]\n",
    "    )\n",
    "    df = preprocessor.fit_transform(df)\n",
    "\n",
    "\n",
    "def normalization():\n",
    "    # Step 2: RobustScaler\n",
    "    robust_scaler_total = RobustScaler()\n",
    "    df['total_deposit_amount_log_scaled'] = robust_scaler_total.fit_transform(\n",
    "        df[['total_deposit_amount_log']]\n",
    "    )\n",
    "\n",
    "    # Step 2: RobustScaler\n",
    "    robust_scaler_item = RobustScaler()\n",
    "    df['item_amt_log_scaled'] = robust_scaler_item.fit_transform(\n",
    "        df[['item_amt_log']]\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def feature_engineer(df):\n",
    "\n",
    "    # drop unused columns\n",
    "    cols_to_drop = ['masked_dep_acct_num', 'masked_id', 'channel']\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # process masked_product_code\n",
    "    top_n = 8\n",
    "    top_product_codes = df['masked_product_code'].value_counts().nlargest(top_n).index\n",
    "    df['masked_product_code_grouped'] = df['masked_product_code'].apply(\n",
    "        lambda x: f'prod_{x}' if x in top_product_codes else 'Other'\n",
    "    )\n",
    "    df = df.drop(columns='masked_product_code')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
